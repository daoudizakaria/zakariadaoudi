<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="Zakaria Daoudi - Linear Regression Algorithm">
    <meta name="author" content="Zakaria Daoudi">
    <title>Zakaria Daoudi | Linear Regression</title>
    <link rel="icon" type="image/x-icon" href="assets/imgs/android-chrome.png" />
    <!-- Font Icons -->
    <link rel="stylesheet" href="assets/vendors/themify-icons/css/themify-icons.css">
    <!-- Bootstrap + Custom Styles -->
    <link rel="stylesheet" href="assets/css/johndoe.css">
    <!-- MathJax for equations -->
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <style>
      body {
        font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
        font-size: 16px;
        line-height: 1.8;
        background-color: #f8f9fa;
        color: #2c3e50;
        padding-bottom: 40px;
      }
      h1, h2, h3 {
        font-family: Georgia, serif;
      }
      .sidebar-toc {
        position: fixed;
        top: 100px;
        left: 0;
        width: 260px;
        background: #ffffff;
        padding: 20px;
        height: calc(100% - 100px);
        overflow-y: auto;
        box-shadow: 2px 0 5px rgba(0,0,0,0.05);
        border-right: 2px solid #bdc3c7;
      }
      .sidebar-toc h2 {
        font-size: 1.4em;
        color: #2980b9;
        margin-bottom: 15px;
        border-bottom: 1px solid #bdc3c7;
        padding-bottom: 10px;
      }
      .sidebar-toc ul {
        list-style: none;
        padding: 0;
      }
      .sidebar-toc li {
        margin: 10px 0;
      }
      .sidebar-toc li a {
        font-size: 1em;
        color: #2980b9;
      }
      .sidebar-toc ul ul {
        margin-left: 15px;
      }
      .main-content {
        margin-left: 280px;
        padding: 20px 40px;
        background: #ffffff;
      }
      .section {
        margin-bottom: 40px;
      }
      .section h2 {
        text-align: center;
        color: #2c3e50;
        margin-bottom: 20px;
        border-bottom: 2px solid #bdc3c7;
        padding-bottom: 10px;
      }
      @media (max-width: 768px) {
        .sidebar-toc {
          position: relative;
          width: 100%;
          height: auto;
          border-right: none;
          border-bottom: 1px solid #bdc3c7;
        }
        .main-content {
          margin-left: 0;
          padding: 10px;
        }
      }
    </style>
</head>
<body>
    <!-- Navigation -->
    <style>
  .navbar-custom {
    background: linear-gradient(45deg, #0B3D91, #1E88E5);
    font-family: 'Open Sans', sans-serif; /* Ensure this font is loaded or change to your preferred font */
  }
  /* Increase spacing between nav items */
  .navbar-custom .nav-item {
    margin-left: 1rem;
    margin-right: 1rem;
  }
  /* Adjust nav link font size, weight, and color */
  .navbar-custom .nav-link {
    color: #ffffff; /* Slightly off-white for improved readability */
    font-size: 1.1rem;
    font-weight: 700;
    transition: color 0.3s ease;
  }
  .navbar-custom .nav-link:hover,
  .navbar-custom .nav-link:focus {
    color: #ffffff; /* Bright white on hover */
  }
  .navbar-custom .navbar-toggler {
    border-color: rgba(240, 240, 240, 240);
  }
  .navbar-custom .navbar-toggler-icon {
    background-image: url("data:image/svg+xml;charset=utf8,%3Csvg viewBox='0 0 30 30' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath stroke='rgba(240, 240, 240, 0.7)' stroke-width='2' stroke-linecap='round' stroke-miterlimit='10' d='M4 7h22M4 15h22M4 23h22'/%3E%3C/svg%3E");
  }
  .navbar-custom .brand-title {
    font-size: 1.25rem;
    font-weight: 700;
    color: #ffffff;
  }
  .navbar-custom small {
    font-size: 0.85rem;
    color: #ffffff;
  }
</style>

<!-- Navbar Markup -->
<nav class="navbar sticky-top navbar-expand-lg navbar-dark navbar-custom"
     data-spy="affix" data-offset-top="510">
  <div class="container">

    <!-- Mobile Toggler Button -->
    <button class="navbar-toggler" type="button" data-toggle="collapse" 
            data-target="#navbarContent" aria-controls="navbarContent" 
            aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <!-- Collapsible Nav Items -->
    <div class="collapse navbar-collapse" id="navbarContent">
      <!-- Left Nav Items -->
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a href="index.html" class="nav-link">Home</a>
        </li>
        <li class="nav-item">
          <a href="index.html#about" class="nav-link">About</a>
        </li>
        <li class="nav-item">
          <a href="index.html#resume" class="nav-link">Resume</a>
        </li>
      </ul>

      <!-- Centered Brand (Always Visible) -->
      <div class="mx-auto text-center">
        <span class="h5 brand-title d-block mb-0">Zakaria Daoudi</span>
        <small>
          Theoretical Physicist | Data Analyst | Technical and Scientific Writing
        </small>
      </div>

      <!-- Right Nav Items -->
      <ul class="navbar-nav ml-auto">
        <li class="nav-item">
          <a href="index.html#portfolio" class="nav-link">Portfolio</a>
        </li>
        <li class="nav-item">
          <a href="index.html#blog" class="nav-link">Blog</a>
        </li>
      </ul>
    </div>

  </div>
</nav>
    
    <div class="sidebar-toc">
      <h2>Table of Contents</h2>
      <ul>
        <li><a href="#introduction">1. Introduction</a></li>
        <li><a href="#maths">2. Mathematical Foundations</a>
          <ul>
            <li><a href="#solution">2.1 Solution and Cost Function</a></li>
            <li><a href="#gradient">2.2 Gradient Descent Algorithm</a></li>
          </ul>
        </li>
        <li><a href="#python">3. Python</a>
          <ul>
            <li><a href="#why-python">3.1 Why Python?</a></li>
            <li><a href="#data-exploration">3.2 Data Exploration</a></li>
            <li><a href="#data-visualization">3.3 Data Visualization</a></li>
            <li><a href="#model-training">3.4 Model Training</a></li>
            <li><a href="#model-predictions">3.5 Model Predictions</a></li>
          </ul>
        </li>
        <li><a href="#applications">4. Real-World Application</a></li>
      </ul>
    </div>
    
    <div class="main-content">
      <h1 class="text-center">Linear Regression Algorithm</h1>
      <<div style="text-align: center;">
        <figure>
            <img src="assets/imgs/SLR.png" style="width:100%">
            <figcaption>Fig. 1: Example of Linear Regression applied for a given training dataset. 
            </br>You can find more explanation in the <a href="ML_SLR.html#"><FONT color="blue">section 4</FONT></a> or you can check the Jupyter Notebook 
                </br> that produced this figure in <a href="https://github.com/zackariadaoudi/Machine-Learning-Projects/'CO2 Cars Emissions'/Carbon_Cars_Emissions_Simple_Linear_Regression.ipynb" target="_blank" rel="noopener noreferrer"><FONT color="blue">my Github</FONT></a> 
                or in my <a href="https://colab.research.google.com/drive/1Wo-N9qNZlV_JlL8u4vKhqtRBcSyHa4yP" target="_blank" rel="noopener noreferrer"><FONT color="blue">research google colab notebook</FONT></a>.
            </figcaption>
          </figure>
        </div>
      <section class="section" id="introduction">
        <h2>Introduction</h2>
        Linear Regression is a foundational statistical technique used to explore the relationship between two continuous variables: a predictor (or independent) variable and a response (or dependent) variable. At its core, SLR aims to establish a linear connection between these two variables through a straight line, which best captures the underlying pattern of the data points.

        </br>The fundamental equation for a Linear Regression model is:
        <script type="text/javascript"
                src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
                </script>
        <div>
            <CENTER>$$ f_{w,b}(x)=w x + b + \epsilon $$</CENTER>
        </div>

Here, \(f_{w,b}\) signifies the response variable we are attempting to predict or explain; 
\(X\) is the predictor variable we are using to forecast \(f_{w,b}\); \(b\) is the y-intercept 
(the value of \(f_{w,b}\) when \(x\) is zero); \(w\) represents the slope of the regression line, 
indicating the change in \(f_{w,b}\) for a unit change in \(x\); and (\epsilon\) denotes the error term, 
capturing the variation in \(f_{w,b}\) that cannot be explained by the linear relationship with \(X\).

</br>Linear Regression functions under two primary assumptions: that there is a linear 
relationship between the input and output variables, and that the residuals 
(the differences between observed and predicted values) follow a normal distribution.

</br>This algorithm is a powerful tool not only for predictive analysis but also for providing insights into 
the strength and nature of the association between the variables. 
For instance, if after analyzing data on advertising spend and sales performance, 
a company discovers a high positive slope, it can infer that higher advert spends usually 
correlate with increased sales. In the world of data science and analytics, 
SLR serves as an essential building block for understanding more complex modeling techniques 
and provides a straightforward approach for making informed decisions based on data.
      </section>
      <section class="section" id="maths">
        <h2>Mathematical Foundations</h2>
        Let's assume we have a collection of labeled points 
        \({(x_i,y_i)}^{N}_{i=1}\), where N is the size of the
        collextion, \(x_i\) is the D-dimensional feature vector of example 
     \(i=1,...,N\), \(y_i\) is a real-valed targed and every 
     feature \(x^{j}_i=1,...,D\), is also a real number. 
        </br> Let's call the function \(f_{w,b}\) the <b>model</b>. The precedent notation of the function \(f\)
        means that our model is parametrized by two values: \(w\) and \(b\), also called <b>weights</b> it can be defined as a linear combination as follow 
        <div>
            <CENTER>$$ f_{w,b}(x)=w x + b $$</CENTER>
        </div>
        where \(w\) is a D-dimensional vector of parameters and b is 
        a real number. An even more simplified and compact notation can be used if we set \(x_0=1\), \(\theta_0=b\), and 
        \(\theta_1=w\)
        <div>
            <CENTER>$$ f_{\theta}(x)=\sum_{i=0}^{D} \theta_i x_i = \theta^T x, $$</CENTER>
        </div>
        One can ask:"How is this relevant?", this is precisely the model used 
        to predict the unkown \(y\) for a given x. Since there is a huge number of different pairs 
         (<b>\(w\)</b>, \(b\)), we have to find the optimale values (<b>\(w^{*}\)</b>, \(b^{*}\)), where the optimal values 
        of the parameters define the model that makes the most accurate predictions. 
    </br>
</br>
        <section class="subsection" id="Maths-1">
                <h5 class="mb-3"><span class="text-danger"></span>2.1 Solution and Cost Function</h5>
        In order to find the optimal values for \(w^{*}\) and \(b^{*}\), we tries to minimize the
        following expression 
        <div>
            <CENTER>$$ \frac{1}{N}\sum_{i=1,...,N}(f_{w,b}(x_i)-y_i)^2 $$</CENTER>
        </div>
        The term \((f_{w,b}(x_i)-y_i)^2\) is called the <b>loss function</b> or <b>cost function</b>.
        That function is a choice of measurement of the model accuracy called <b>squared error loss</b>. The cost function is something 
        general to all machine learning algorithms, and the principle is the same
        <CENTER><i><b><FONT size="4pt" color="red">The less the cost function is, the best the model is.</FONT></b></i></CENTER>
        In linear regression, the cost function is given by an average, the <b>average loss</b>, it's basically the average of all penalties
        obtained by apllying the model to the training data. One of the advantages of the linear models and its simplicity 
        beside the fact that <b>it works</b>, is that it rarely overfit. By <b>overfitting</b>, we mean that the model predicts very well labels 
        of the examples used during training but frequently makes errors when applied to the test set. To answer a not asked question,
        The absolute value is not convinient for gauging the quality of the model, because it doesn't have a continuous derivative, hence the function produced 
        is not smooth, therefore, employing linear algebra to find closed form solutions to optimization problems is not easy and can be extremely exchausting. For this reason we use 
        the squaring error before summing, thanks to a wonderful French mathematician from the 18th century called Adrien-Marie Legendre. 
        Closed form solutions to finding an optimum of a function are simple algebraic expressions and are often preferabke to using complex numerical optimization
        methods such as <b>gradient descent</b>.
        </section> 
    </br>
        <section class="subsection" id="Maths-2">
            <h5 class="mb-3"><span class="text-danger"></span>2.2 Gradient Descent Algorithm</h5>
            We start with an initial \(\theta\) value, and we constantly performs the operations
            <div>
                <CENTER>$$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} f_{\theta} $$</CENTER>
            </div>
            where \(\alpha\) is the <b>learning rate</b>. If we suppose that we have only one training example (x,y), the derivative \(\frac{\partial}{\partial \theta_i} J(\theta)\) can be written as 
            <div>
                <CENTER>$$ \frac{\partial}{\partial \theta_i} J(\theta) = \frac{\partial}{\partial \theta_i} \frac{1}{2} (h_{\theta}(x)-y)^2 = (h_{\theta}(x) - y) x_j $$</CENTER>
            </div>
            Hence the operation can be rewritten for a signle training example as 
            <div>
                <center>$$ \theta_j := \theta_j + \alpha (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}$$</center>
            </div>
            This is the <b>Least Mean Squares</b> rule, which is also known as the <b>Widrow-Hoff</b> learning rule. For training sets with more than one example, we can repeat the LMS rule until convergence,
            <b>i.e.</b> repeat it for every \(j\), or we group the coordinates into a vector called \(\theta\),
            <div>
                <center>$$ \theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)}$$</center>
            </div>
        </section> 
     </div>
      </section>
      <section class="section" id="python">
        <h2>Python for Linear Regression</h2>
        <p>Python provides a variety of libraries such as NumPy, pandas, and scikit-learn that make implementing linear regression simple and efficient...</p>
      </section>
      <section class="section" id="applications">
        <h2>Real-World Applications</h2>
        <p>Linear regression is widely used in economics, finance, machine learning, and various data-driven industries...</p>
      </section>
    </div>
</body>
</html>

